================================================================
EXPLICATION INTENSIVE DES TECHNOLOGIES DU PROJET MLOPS
================================================================

1. DOCKER & DOCKER COMPOSE
----------------------------------------------------------------
> QU'EST-CE QUE C'EST ?
Docker est une plateforme de "conteneurisation". Imaginez des "boÃ®tes" virtuelles lÃ©gÃ¨res qui contiennent tout ce dont votre application a besoin pour fonctionner (code, librairies, version de Python, systÃ¨me d'exploitation minimal).

> POURQUOI L'UTILISER ?
Le problÃ¨me classique en informatique est : "Ã‡a marche sur ma machine, mais pas sur la tienne". Docker rÃ©sout Ã§a. Si Ã§a marche dans le conteneur Docker, Ã§a marchera partout (Windows, Linux, Mac, Cloud).

> DANS CE PROJET :
Toute notre infrastructure tourne dans des conteneurs isolÃ©s dÃ©finis dans le fichier `docker-compose.yaml`.
- Nous avons 4 conteneurs qui tournent en mÃªme temps :
  1. Postgres (Base de donnÃ©es)
  2. MLflow (Serveur de tracking)
  3. Airflow Webserver (Interface)
  4. Airflow Scheduler (Moteur)
Docker Compose permet de lancer ces 4 services avec une seule commande : `docker-compose up`.

2. APACHE AIRFLOW
----------------------------------------------------------------
> QU'EST-CE QUE C'EST ?
C'est un outil d'orchestration de workflow. Il permet de programmer, planifier et surveiller des sÃ©quences de tÃ¢ches complexes (appelÃ©es DAGs - Directed Acyclic Graphs).

> POURQUOI L'UTILISER ?
En Data Science, on a souvent des pipelines : "TÃ©lÃ©charger les donnÃ©es" -> "Nettoyer" -> "EntraÃ®ner" -> "Evaluer". Faire Ã§a Ã  la main est fastidieux et source d'erreurs. Airflow automatise tout Ã§a.

> DANS CE PROJET :
Airflow est le "Cerveau" du systÃ¨me.
- Il exÃ©cute le script `continuous_retraining_dag.py` tous les jours.
- Il gÃ¨re la logique : "Si nouvelles donnÃ©es -> Alors entraÃ®ner -> Si meilleur -> Alors dÃ©ployer".
- Il gÃ¨re les erreurs : si une Ã©tape plante, il peut rÃ©essayer ou vous envoyer une alerte.

3. MLFLOW
----------------------------------------------------------------
> QU'EST-CE QUE C'EST ?
C'est une plateforme open-source pour gÃ©rer le cycle de vie du Machine Learning. Elle permet de "tracker" (suivre) chaque expÃ©rience.

> POURQUOI L'UTILISER ?
Quand on entraÃ®ne un modÃ¨le, on change souvent des paramÃ¨tres (learning rate, epochs...). On finit par avoir 50 versions et on ne sait plus laquelle est la meilleure. MLflow note tout pour vous.

> DANS CE PROJET :
MLflow agit comme le "Carnet de notes" du laboratoire.
- Ã€ chaque entraÃ®nement lancÃ© par Airflow, MLflow enregistre :
  - Les hyperparamÃ¨tres utilisÃ©s.
  - La prÃ©cision (Accuracy) obtenue.
  - Le fichier du modÃ¨le (.pth).
- C'est grÃ¢ce Ã  lui qu'on peut comparer l'ancien et le nouveau modÃ¨le pour dÃ©cider si on dÃ©ploie.

4. DVC (DATA VERSION CONTROL)
----------------------------------------------------------------
> QU'EST-CE QUE C'EST ?
C'est "Git pour les donnÃ©es". Git est gÃ©nial pour le code (texte lÃ©ger), mais nul pour les gros fichiers (images, vidÃ©os). DVC permet de versionner des gigaoctets de donnÃ©es tout en utilisant Git.

> POURQUOI L'UTILISER ?
Pour la reproductibilitÃ©. Si vous rÃ©-entraÃ®nez un modÃ¨le dans 6 mois, vous devez Ãªtre sÃ»r d'utiliser EXACTEMENT les mÃªmes donnÃ©es qu'aujourd'hui.

> DANS CE PROJET :
DVC suit notre dossier d'images `data/chest_xray`.
- Le fichier `chest_xray.dvc` est un petit fichier texte qui contient le "hash" (l'empreinte digitale) des donnÃ©es.
- Ce petit fichier est partagÃ© sur GitHub.
- Les vraies images sont stockÃ©es Ã  part (cache local ou cloud), et DVC les "tÃ©lÃ©charge" quand on en a besoin.

5. PYTORCH (TORCH & TORCHVISION)
----------------------------------------------------------------
> QU'EST-CE QUE C'EST ?
C'est un framework de Deep Learning dÃ©veloppÃ© par Facebook (Meta). Il permet de crÃ©er et d'entraÃ®ner des rÃ©seaux de neurones.

> POURQUOI L'UTILISER ?
Il est trÃ¨s flexible, facile Ã  dÃ©bugger, et trÃ¨s populaire dans la recherche.

> DANS CE PROJET :
- Nous utilisons le modÃ¨le **ResNet18** (Residual Network Ã  18 couches).
- Nous utilisons le **Transfer Learning** : on prend un ResNet18 dÃ©jÃ  intelligent (entraÃ®nÃ© sur ImageNet) et on lui apprend juste Ã  reconnaÃ®tre la pneumonie. Cela permet d'avoir d'excellents rÃ©sultats avec peu de donnÃ©es et un entraÃ®nement rapide.

6. DJANGO
----------------------------------------------------------------
> QU'EST-CE QUE C'EST ?
C'est un framework web complet en Python ("Batteries included"). Il permet de crÃ©er des sites web robustes et sÃ©curisÃ©s rapidement.

> POURQUOI L'UTILISER ?
Pour transformer notre modÃ¨le mathÃ©matique en une vraie application utilisable par des humains. Un mÃ©decin ne veut pas lancer un script Python, il veut un site web.

> DANS CE PROJET :
- Django sert l'interface graphique (HTML/CSS).
- Il gÃ¨re l'upload sÃ©curisÃ© des images radios.
- Il charge le modÃ¨le PyTorch en arriÃ¨re-plan et fait la prÃ©diction quand on clique sur "Analyser".

7. HEROKU
----------------------------------------------------------------
> QU'EST-CE QUE C'EST ?
C'est une plateforme Cloud (PaaS - Platform as a Service). Elle permet de mettre un site web en ligne sans gÃ©rer de serveurs complexes.

> POURQUOI L'UTILISER ?
Pour rendre le projet accessible au monde entier. C'est l'Ã©tape finale du MLOps : le dÃ©ploiement en production.

> DANS CE PROJET :
- Heroku hÃ©berge notre code Django.
- Il redÃ©marre l'application automatiquement quand Airflow lui envoie une nouvelle version du modÃ¨le via Git (`git push heroku master`).

8. POSTGRESQL
----------------------------------------------------------------
> QU'EST-CE QUE C'EST ?
C'est un systÃ¨me de gestion de base de donnÃ©es relationnelle trÃ¨s puissant et fiable.

> DANS CE PROJET :
C'est la base de donnÃ©es technique utilisÃ©e par Airflow et MLflow pour stocker leurs informations internes (historique des jobs, utilisateurs, configurations, mÃ©tadonnÃ©es des runs).

================================================================
RÃ‰SUMÃ‰ DE L'INTERACTION
================================================================
1. Le Code est sur **GitHub**.
2. Les DonnÃ©es sont gÃ©rÃ©es par **DVC**.
3. **Airflow** (dans **Docker**) dÃ©tecte les changements.
4. Il lance **PyTorch** pour entraÃ®ner.
5. Il loggue les rÃ©sultats dans **MLflow**.
6. Si c'est bon, il dÃ©ploie le tout sur **Heroku** via **Git**.
7. L'utilisateur final utilise **Django**.
================================================================

9. CI/CD (CONTINUOUS INTEGRATION / CONTINUOUS DELIVERY)
----------------------------------------------------------------
> QU'EST-CE QUE C'EST ?
Ce sont deux pratiques devops fondamentales pour automatiser la mise en production.

> CI (CONTINUOUS INTEGRATION) :
C'est le fait de fusionner régulièrement le code de tous les développeurs dans une branche commune.
- But : Détecter les bugs très vite grâce à des tests automatiques.
- Dans ce projet : C'est l'utilisation de **Git** et **GitHub**. Quand vous faites un git push, vous intégrez votre travail. Si on avait une équipe, GitHub vérifierait que votre nouveau code ne casse pas le code des autres.

> CD (CONTINUOUS DELIVERY / DEPLOYMENT) :
C'est l'automatisation de la mise en production. Une fois que le code est intégré (CI) et validé, il est automatiquement envoyé aux utilisateurs.
- But : Rendre les nouvelles fonctionnalités disponibles immédiatement, sans intervention humaine manuelle et risquée.
- Dans ce projet : C'est le rôle d'**Airflow** et **Heroku**.
  1. Airflow valide que le nouveau modèle est meilleur (Test).
  2. Si oui, il déclenche le déploiement sur Heroku (Delivery).
  3. L'application est mise à jour instantanément pour les médecins.

> RÉSUMÉ SIMPLE :
- **CI** = J'intègre mon code proprement (Git).
- **CD** = Je livre le produit fini automatiquement (Heroku).

